{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ase\n",
    "from ase.io import read, write\n",
    "import numpy as np\n",
    "import scipy\n",
    "from ase.build import molecule, bulk\n",
    "from ase.neighborlist import neighbor_list\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_nl.ase_impl import ase_neighbor_list\n",
    "from torch_nl.torch_impl import compute_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n^2 impl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3]),\n",
       " tensor([ 6, 13,  8,  8]),\n",
       " torch.Size([35, 3]),\n",
       " torch.Size([12, 3]),\n",
       " torch.Size([12]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [molecule('OCHCHO'), molecule('C3H9C')]\n",
    "# frames = [molecule('C3H9C')]\n",
    "frames += [bulk('Si', 'diamond', a=6, cubic=True)]\n",
    "frames += [bulk('Si', 'diamond', a=4, cubic=True)]\n",
    "# frames = [bulk('Si', 'diamond', a=6.1, cubic=True)]\n",
    "rcut = 3\n",
    "\n",
    "# pos = torch.from_numpy(frame.get_positions())\n",
    "# cell = torch.from_numpy(frame.get_cell().array)\n",
    "# pbc = torch.from_numpy(frame.get_pbc())\n",
    "n_atoms = [0]\n",
    "pos = []\n",
    "cell = []\n",
    "pbc = []\n",
    "for ff in frames:\n",
    "    n_atoms.append(len(ff))\n",
    "    pos.append(torch.from_numpy(ff.get_positions()))\n",
    "    cell.append(torch.from_numpy(ff.get_cell().array))\n",
    "    pbc.append(torch.from_numpy(ff.get_pbc()))\n",
    "pos = torch.cat(pos)\n",
    "cell = torch.cat(cell)\n",
    "pbc = torch.cat(pbc)\n",
    "stride = torch.from_numpy(np.cumsum(n_atoms))\n",
    "batch = torch.zeros(pos.shape[0],dtype=torch.long)\n",
    "for ii,(st,nd) in enumerate(zip(stride[:-1],stride[1:])):\n",
    "    batch[st:nd] = ii\n",
    "n_atoms = torch.Tensor(n_atoms[1:]).to(dtype=torch.long)\n",
    "batch,n_atoms, pos.shape, cell.shape, pbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = []\n",
    "mm = []\n",
    "for frame in frames:\n",
    "    idx_i, idx_j, idx_S, dd = neighbor_list(\n",
    "            \"ijSd\", frame, cutoff=rcut, self_interaction=False\n",
    "        )\n",
    "    dist.append(np.sort(dd))\n",
    "    mm.append((idx_i, idx_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "code_folding": [
     0,
     17,
     24
    ]
   },
   "outputs": [],
   "source": [
    "def get_fully_connected_mapping(i_ids, shifts_idx) -> torch.Tensor:\n",
    "    n_atom = i_ids.shape[0]\n",
    "    n_atom2 = n_atom * n_atom\n",
    "    n_cell_image = shifts_idx.shape[0]\n",
    "    j_ids = torch.repeat_interleave(i_ids, n_cell_image)\n",
    "    mapping = torch.cartesian_prod(i_ids, j_ids)\n",
    "    shifts_idx = shifts_idx.repeat((n_atom2, 1))\n",
    "    \n",
    "    mask = torch.ones(mapping.shape[0], dtype=bool, device=i_ids.device)\n",
    "    ids = n_cell_image*torch.arange(n_atom, device=i_ids.device) \\\n",
    "                + torch.arange(0, mapping.shape[0], n_atom*n_cell_image, device=i_ids.device)\n",
    "    # print(n_atom*n_cell_image, ids)\n",
    "    mask[ids] = False\n",
    "    mapping = mapping[mask, :]\n",
    "    shifts_idx = shifts_idx[mask]\n",
    "    return mapping, shifts_idx\n",
    "\n",
    "def compute_images(\n",
    "    positions: torch.Tensor,\n",
    "    cell: torch.Tensor,\n",
    "    pbc: torch.Tensor,\n",
    "    cutoff: float,\n",
    "    batch: torch.Tensor,\n",
    "    n_atoms: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"TODO: add doc\"\"\"\n",
    "    cell = cell.view((-1, 3, 3)).to(torch.float32)\n",
    "    pbc = pbc.view((-1, 3))\n",
    "    dtype = cell.dtype\n",
    "    has_pbc = pbc.prod(dim=1, dtype=bool)\n",
    "    stride = torch.zeros(n_atoms.shape[0]+1,dtype=torch.long)\n",
    "    stride[1:] = torch.cumsum(n_atoms, dim=0,dtype=torch.long)\n",
    "    reciprocal_cell = torch.zeros_like(cell)\n",
    "    reciprocal_cell[has_pbc,:,:] = torch.linalg.inv(cell[has_pbc,:,:]).transpose(2, 1)\n",
    "    inv_distances = reciprocal_cell.norm(2, dim=-1)\n",
    "    num_repeats = torch.ceil(cutoff * inv_distances).to(torch.long)\n",
    "    num_repeats_ = torch.where(pbc, num_repeats, torch.zeros_like(num_repeats))\n",
    "    ids = torch.arange(positions.shape[0], device=positions.device, dtype=torch.long)\n",
    "    images, mapping, batch_mapping, shifts_expanded, shifts_idx_ = [], [], [], [], []\n",
    "    for i_structure in range(n_atoms.shape[0]):\n",
    "        num_repeats = num_repeats_[i_structure]\n",
    "        reps = []\n",
    "        for ii in range(3):\n",
    "            r1 = torch.arange(\n",
    "                -num_repeats[ii],\n",
    "                num_repeats[ii] + 1,\n",
    "                device=cell.device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            _, indices = torch.sort(torch.abs(r1))\n",
    "            reps.append(r1[indices])\n",
    "        shifts_idx = torch.cartesian_prod(*reps)\n",
    "        n_cell_image = shifts_idx.shape[0]\n",
    "        n_atom = n_atoms[i_structure]\n",
    "        pos = positions[stride[i_structure]:stride[i_structure+1]]\n",
    "        i_ids = ids[stride[i_structure]:stride[i_structure+1]]\n",
    "        \n",
    "        s_mapping, shifts_idx = get_fully_connected_mapping(i_ids, shifts_idx)\n",
    "        mapping.append(s_mapping)\n",
    "        batch_mapping.append(\n",
    "            i_structure\n",
    "            * torch.ones(\n",
    "                s_mapping.shape[0], dtype=torch.long, device=cell.device\n",
    "            )\n",
    "        )\n",
    "        shifts_idx_.append(shifts_idx)\n",
    "    return (\n",
    "        torch.cat(mapping, dim=0).t(),\n",
    "        torch.cat(batch_mapping, dim=0),\n",
    "        torch.cat(shifts_idx_, dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def compute_distances(\n",
    "    pos: torch.Tensor,\n",
    "    mapping: torch.Tensor,\n",
    "    cell_shifts: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    assert mapping.dim() == 2\n",
    "    assert mapping.shape[0] == 2\n",
    "\n",
    "    if cell_shifts is None:\n",
    "        dr = pos[mapping[1]] - pos[mapping[0]]\n",
    "    else:\n",
    "        dr = pos[mapping[1]] - pos[mapping[0]] + cell_shifts\n",
    "\n",
    "    return dr.norm(p=2, dim=1)\n",
    "\n",
    "def compute_cell_shifts(cell, shifts_idx, batch_mapping):\n",
    "    if cell is None:\n",
    "        cell_shifts = None\n",
    "    else:\n",
    "        cell_shifts = torch.einsum(\"jn,jnm->jm\", shifts_idx, cell.view(-1, 3, 3)[batch_mapping])\n",
    "    return cell_shifts\n",
    "\n",
    "def compute_strict_nl_n2(pos, cell, mapping, batch_mapping,  shifts_idx):\n",
    "    cell_shifts = compute_cell_shifts(cell, shifts_idx, batch_mapping)\n",
    "    d2 = (pos[mapping[0]] - pos[mapping[1]] - cell_shifts).square().sum(dim=1)\n",
    "    mask = d2 <= rcut*rcut\n",
    "    mapping = mapping[:, mask]\n",
    "    mapping_batch = batch_mapping[mask]\n",
    "    shifts_idx = shifts_idx[mask]\n",
    "    return mapping, mapping_batch, shifts_idx # , d2[mask].sqrt()\n",
    "\n",
    "def compute_nl(pos, cell, pbc, rcut, batch, method='n2'):\n",
    "    n_atoms = torch.bincount(batch)\n",
    "    mapping, batch_mapping, shifts_idx = compute_images(pos, \n",
    "                                     cell, pbc, rcut, \n",
    "                                     batch, \n",
    "                                     n_atoms)\n",
    "    if method == 'n2':\n",
    "        mapping, mapping_batch, shifts_idx = compute_strict_nl_n2(pos, cell, mapping, batch_mapping,  shifts_idx)\n",
    "    \n",
    "    return mapping, mapping_batch, shifts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [222]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mapping, batch_mapping, shifts_idx \u001b[38;5;241m=\u001b[39m compute_images(pos, \n\u001b[1;32m      2\u001b[0m                                      cell, pbc, rcut, \n\u001b[1;32m      3\u001b[0m                                      batch, \n\u001b[1;32m      4\u001b[0m                                      n_atoms)\n\u001b[0;32m----> 6\u001b[0m cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cell_shifts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m d2 \u001b[38;5;241m=\u001b[39m (pos[mapping[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m-\u001b[39m pos[mapping[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m-\u001b[39m cell_shifts )\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m mask \u001b[38;5;241m=\u001b[39m d2 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcut\u001b[38;5;241m*\u001b[39mrcut\n",
      "Input \u001b[0;32mIn [221]\u001b[0m, in \u001b[0;36mcompute_cell_shifts\u001b[0;34m(cell, shifts_idx, batch_mapping)\u001b[0m\n\u001b[1;32m     19\u001b[0m     cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjn,jnm->jm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_mapping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_shifts\n",
      "File \u001b[0;32m/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/torch/functional.py:330\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "mapping, batch_mapping, shifts_idx = compute_images(pos, \n",
    "                                     cell, pbc, rcut, \n",
    "                                     batch, \n",
    "                                     n_atoms)\n",
    "\n",
    "cell_shifts = compute_cell_shifts(cell, shifts_idx, batch_mapping)\n",
    "d2 = (pos[mapping[0]] - pos[mapping[1]] - cell_shifts ).square().sum(dim=1)\n",
    "mask = d2 <= rcut*rcut\n",
    "d = d2[mask].sqrt()\n",
    "mapping = mapping[:, mask]\n",
    "mapping_batch = batch_mapping[mask]\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26,),\n",
       " array([1.10563938, 1.10563938, 1.10563938, 1.10563938, 1.22295086,\n",
       "        1.22295086, 1.22295086, 1.22295086, 1.51286   , 1.51286   ,\n",
       "        2.05241169, 2.05241169, 2.05241169, 2.05241169, 2.22229623,\n",
       "        2.22229623, 2.22229623, 2.22229623, 2.38769877, 2.38769877,\n",
       "        2.38769877, 2.38769877, 2.61851372, 2.61851372, 2.61851372,\n",
       "        2.61851372]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0].shape,dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([216, 3]),\n",
       " torch.Size([35, 3]),\n",
       " torch.Size([2, 288]),\n",
       " torch.Size([216, 3]),\n",
       " torch.Size([3626, 3]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, pos.shape, mapping.shape, shifts_expanded.shape, shifts_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for ii, dd in enumerate(dist):\n",
    "    print(np.allclose(np.sort(dd), np.sort(d[mapping_batch == ii].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1977, dtype=torch.float64), 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.min(), rcut*rcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32,), torch.Size([23]), torch.Size([1720]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0].shape, d.shape, d2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 23]),\n",
       " torch.Size([216, 3]),\n",
       " tensor([[ 0,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,\n",
       "           6,  7,  7,  7,  7],\n",
       "         [ 1,  0,  2,  4,  6,  1,  3,  2, 14, 28, 32,  1,  5,  4, 14, 74, 80,  1,\n",
       "           7,  6, 28, 74, 96]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping.shape, images.shape, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.arange(3).reshape(3,1)\n",
    "torch.cat([aa,aa], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.arange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "         4, 5, 5, 5, 5, 5],\n",
       "        [1, 2, 3, 4, 5, 0, 2, 3, 4, 5, 0, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 1, 2, 3,\n",
       "         5, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linked cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_nl import"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cg39",
   "language": "python",
   "name": "cg39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48.9333px",
    "width": "251.8px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f79d3df5ff5684964744ab9f5218f96eb946f2b40d3f02d5eb965bb50f364a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
