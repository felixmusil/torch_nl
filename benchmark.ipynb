{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ase\n",
    "import numpy as np\n",
    "import scipy\n",
    "from ase.build import molecule, bulk, make_supercell\n",
    "from ase.neighborlist import neighbor_list\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_nl import compute_neighborlist, compute_neighborlist_n2, ase2data\n",
    "from torch_nl.timer import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Periodic systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "##  Metal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = bulk('Si', 'diamond', a=4, cubic=True)\n",
    "aa = torch.arange(1, 4)\n",
    "Ps = torch.cartesian_prod(aa,aa,aa)\n",
    "Ps = Ps[torch.sort(Ps.sum(dim=1)).indices].to(torch.long).numpy()\n",
    "frames = []\n",
    "n_atoms = []\n",
    "for P in Ps:\n",
    "    frames.append(make_supercell(frame, np.diag(P)))\n",
    "    n_atoms.append(len(frames[-1]))\n",
    "n_atoms = np.array(n_atoms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cutoff = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763badae543742b09318b719c2155284",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tag = \"ASE\"\n",
    "datas = []\n",
    "for frame in tqdm(frames):\n",
    "    timing = timeit(neighbor_list, ['ijS', frame, cutoff], tag=tag, warmup=1, nit=50)\n",
    "    data = timing.dumps()\n",
    "    i,j,S = neighbor_list('ijS', frame, cutoff)\n",
    "    n_neighbor = np.bincount(i).mean()\n",
    "    data.update(n_atom=len(frame), n_neighbor_per_atom_avg=int(n_neighbor))\n",
    "    data.pop('samples')\n",
    "    datas.append(data)\n",
    "# df = pd.DataFrame(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eb9836f944247b8845d579c0597e3a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdf65454d8694559bb43a0601e05ece1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/27 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tags = [\n",
    "    # \"torch_nl O(n^2) CPU\", \n",
    "    # \"torch_nl O(n^2) GPU\", \n",
    "    \"torch_nl O(n) CPU\", \n",
    "    # \"torch_nl O(n) GPU\"\n",
    "]\n",
    "for tag in tqdm(tags):\n",
    "    if \"CPU\" in tag:\n",
    "        device = 'cpu'\n",
    "    elif \"GPU\" in tag:\n",
    "        device = 'cuda'\n",
    "        \n",
    "    if 'O(n^2)' in tag:\n",
    "        nl_func = compute_neighborlist_n2\n",
    "    elif 'O(n)' in tag:\n",
    "        nl_func = compute_neighborlist\n",
    "\n",
    "    for frame in tqdm(frames):\n",
    "        pos, cell, pbc, batch, n_atoms = ase2data([frame], device=device)\n",
    "        timing = timeit(nl_func, [cutoff, pos, cell, pbc, batch], tag=tag, warmup=10, nit=50)\n",
    "        data = timing.dumps()\n",
    "        data.pop('samples')\n",
    "        mapping, mapping_batch, shifts_idx = nl_func(cutoff, pos, cell, pbc, batch)\n",
    "        n_neighbor = np.bincount(mapping[0].cpu().numpy()).mean()\n",
    "        data.update(n_atom=len(frame), n_neighbor_per_atom_avg=int(n_neighbor))\n",
    "        datas.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tag</th>\n",
       "      <th>mean</th>\n",
       "      <th>stdev</th>\n",
       "      <th>min</th>\n",
       "      <th>max</th>\n",
       "      <th>n_atom</th>\n",
       "      <th>n_neighbor_per_atom_avg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.002439</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>0.002380</td>\n",
       "      <td>0.002541</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.003136</td>\n",
       "      <td>0.000131</td>\n",
       "      <td>0.003051</td>\n",
       "      <td>0.003652</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.003092</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>0.003271</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.003084</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.003053</td>\n",
       "      <td>0.003487</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.002837</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>0.002803</td>\n",
       "      <td>0.002966</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004419</td>\n",
       "      <td>0.000118</td>\n",
       "      <td>0.004324</td>\n",
       "      <td>0.005133</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.002805</td>\n",
       "      <td>0.003203</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.002823</td>\n",
       "      <td>0.000026</td>\n",
       "      <td>0.002797</td>\n",
       "      <td>0.002942</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004364</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>0.004574</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004365</td>\n",
       "      <td>0.000056</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>0.004600</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.007708</td>\n",
       "      <td>0.000042</td>\n",
       "      <td>0.007667</td>\n",
       "      <td>0.007855</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004203</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.004185</td>\n",
       "      <td>0.004255</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004198</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.004177</td>\n",
       "      <td>0.004307</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004189</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.004169</td>\n",
       "      <td>0.004286</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004186</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.004170</td>\n",
       "      <td>0.004369</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004249</td>\n",
       "      <td>0.000246</td>\n",
       "      <td>0.004179</td>\n",
       "      <td>0.005684</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004211</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.004184</td>\n",
       "      <td>0.004468</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006491</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.006472</td>\n",
       "      <td>0.006516</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006482</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.006457</td>\n",
       "      <td>0.006738</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004301</td>\n",
       "      <td>0.000009</td>\n",
       "      <td>0.004278</td>\n",
       "      <td>0.004330</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004295</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.004277</td>\n",
       "      <td>0.004331</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.006445</td>\n",
       "      <td>0.006659</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.004298</td>\n",
       "      <td>0.000012</td>\n",
       "      <td>0.004282</td>\n",
       "      <td>0.004354</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006723</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.006703</td>\n",
       "      <td>0.006785</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006694</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.006673</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.006700</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.006682</td>\n",
       "      <td>0.006731</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>ASE</td>\n",
       "      <td>0.009270</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.009233</td>\n",
       "      <td>0.009443</td>\n",
       "      <td>216</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.001825</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.001810</td>\n",
       "      <td>0.001843</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.003014</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.003052</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.003043</td>\n",
       "      <td>0.000143</td>\n",
       "      <td>0.002972</td>\n",
       "      <td>0.003826</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.003039</td>\n",
       "      <td>0.000109</td>\n",
       "      <td>0.002968</td>\n",
       "      <td>0.003720</td>\n",
       "      <td>16</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.004160</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.004116</td>\n",
       "      <td>0.004339</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.005311</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.005231</td>\n",
       "      <td>0.005824</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.004171</td>\n",
       "      <td>0.000064</td>\n",
       "      <td>0.004115</td>\n",
       "      <td>0.004439</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.004105</td>\n",
       "      <td>0.004399</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.005292</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.005229</td>\n",
       "      <td>0.005457</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.005296</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.005246</td>\n",
       "      <td>0.005449</td>\n",
       "      <td>32</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.009644</td>\n",
       "      <td>0.000186</td>\n",
       "      <td>0.009496</td>\n",
       "      <td>0.010708</td>\n",
       "      <td>64</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007527</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.007771</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007529</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.007462</td>\n",
       "      <td>0.007826</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007518</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.007442</td>\n",
       "      <td>0.007618</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007480</td>\n",
       "      <td>0.000035</td>\n",
       "      <td>0.007405</td>\n",
       "      <td>0.007558</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007534</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.007455</td>\n",
       "      <td>0.007692</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.007492</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.007431</td>\n",
       "      <td>0.007568</td>\n",
       "      <td>48</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.013919</td>\n",
       "      <td>0.000054</td>\n",
       "      <td>0.013798</td>\n",
       "      <td>0.014047</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.013915</td>\n",
       "      <td>0.000058</td>\n",
       "      <td>0.013818</td>\n",
       "      <td>0.014079</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.010648</td>\n",
       "      <td>0.000052</td>\n",
       "      <td>0.010557</td>\n",
       "      <td>0.010792</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.010625</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.010551</td>\n",
       "      <td>0.010707</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.013865</td>\n",
       "      <td>0.000044</td>\n",
       "      <td>0.013759</td>\n",
       "      <td>0.013966</td>\n",
       "      <td>96</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.010629</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.010547</td>\n",
       "      <td>0.010737</td>\n",
       "      <td>72</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.020425</td>\n",
       "      <td>0.000069</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>0.020624</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.020429</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.020302</td>\n",
       "      <td>0.020571</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.020426</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.020236</td>\n",
       "      <td>0.020575</td>\n",
       "      <td>144</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>torch_nl O(n) CPU</td>\n",
       "      <td>0.029750</td>\n",
       "      <td>0.000132</td>\n",
       "      <td>0.029452</td>\n",
       "      <td>0.030134</td>\n",
       "      <td>216</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  tag      mean     stdev       min       max  n_atom  \\\n",
       "0                 ASE  0.002439  0.000027  0.002380  0.002541       8   \n",
       "1                 ASE  0.003136  0.000131  0.003051  0.003652      16   \n",
       "2                 ASE  0.003092  0.000048  0.003052  0.003271      16   \n",
       "3                 ASE  0.003084  0.000062  0.003053  0.003487      16   \n",
       "4                 ASE  0.002837  0.000034  0.002803  0.002966      24   \n",
       "5                 ASE  0.004419  0.000118  0.004324  0.005133      32   \n",
       "6                 ASE  0.002838  0.000062  0.002805  0.003203      24   \n",
       "7                 ASE  0.002823  0.000026  0.002797  0.002942      24   \n",
       "8                 ASE  0.004364  0.000048  0.004330  0.004574      32   \n",
       "9                 ASE  0.004365  0.000056  0.004331  0.004600      32   \n",
       "10                ASE  0.007708  0.000042  0.007667  0.007855      64   \n",
       "11                ASE  0.004203  0.000012  0.004185  0.004255      48   \n",
       "12                ASE  0.004198  0.000018  0.004177  0.004307      48   \n",
       "13                ASE  0.004189  0.000016  0.004169  0.004286      48   \n",
       "14                ASE  0.004186  0.000028  0.004170  0.004369      48   \n",
       "15                ASE  0.004249  0.000246  0.004179  0.005684      48   \n",
       "16                ASE  0.004211  0.000048  0.004184  0.004468      48   \n",
       "17                ASE  0.006491  0.000009  0.006472  0.006516      96   \n",
       "18                ASE  0.006482  0.000045  0.006457  0.006738      96   \n",
       "19                ASE  0.004301  0.000009  0.004278  0.004330      72   \n",
       "20                ASE  0.004295  0.000008  0.004277  0.004331      72   \n",
       "21                ASE  0.006466  0.000037  0.006445  0.006659      96   \n",
       "22                ASE  0.004298  0.000012  0.004282  0.004354      72   \n",
       "23                ASE  0.006723  0.000016  0.006703  0.006785     144   \n",
       "24                ASE  0.006694  0.000008  0.006673  0.006717     144   \n",
       "25                ASE  0.006700  0.000011  0.006682  0.006731     144   \n",
       "26                ASE  0.009270  0.000040  0.009233  0.009443     216   \n",
       "27  torch_nl O(n) CPU  0.001825  0.000007  0.001810  0.001843       8   \n",
       "28  torch_nl O(n) CPU  0.003014  0.000019  0.002971  0.003052      16   \n",
       "29  torch_nl O(n) CPU  0.003043  0.000143  0.002972  0.003826      16   \n",
       "30  torch_nl O(n) CPU  0.003039  0.000109  0.002968  0.003720      16   \n",
       "31  torch_nl O(n) CPU  0.004160  0.000035  0.004116  0.004339      24   \n",
       "32  torch_nl O(n) CPU  0.005311  0.000101  0.005231  0.005824      32   \n",
       "33  torch_nl O(n) CPU  0.004171  0.000064  0.004115  0.004439      24   \n",
       "34  torch_nl O(n) CPU  0.004163  0.000054  0.004105  0.004399      24   \n",
       "35  torch_nl O(n) CPU  0.005292  0.000035  0.005229  0.005457      32   \n",
       "36  torch_nl O(n) CPU  0.005296  0.000036  0.005246  0.005449      32   \n",
       "37  torch_nl O(n) CPU  0.009644  0.000186  0.009496  0.010708      64   \n",
       "38  torch_nl O(n) CPU  0.007527  0.000060  0.007442  0.007771      48   \n",
       "39  torch_nl O(n) CPU  0.007529  0.000057  0.007462  0.007826      48   \n",
       "40  torch_nl O(n) CPU  0.007518  0.000037  0.007442  0.007618      48   \n",
       "41  torch_nl O(n) CPU  0.007480  0.000035  0.007405  0.007558      48   \n",
       "42  torch_nl O(n) CPU  0.007534  0.000048  0.007455  0.007692      48   \n",
       "43  torch_nl O(n) CPU  0.007492  0.000030  0.007431  0.007568      48   \n",
       "44  torch_nl O(n) CPU  0.013919  0.000054  0.013798  0.014047      96   \n",
       "45  torch_nl O(n) CPU  0.013915  0.000058  0.013818  0.014079      96   \n",
       "46  torch_nl O(n) CPU  0.010648  0.000052  0.010557  0.010792      72   \n",
       "47  torch_nl O(n) CPU  0.010625  0.000036  0.010551  0.010707      72   \n",
       "48  torch_nl O(n) CPU  0.013865  0.000044  0.013759  0.013966      96   \n",
       "49  torch_nl O(n) CPU  0.010629  0.000038  0.010547  0.010737      72   \n",
       "50  torch_nl O(n) CPU  0.020425  0.000069  0.020302  0.020624     144   \n",
       "51  torch_nl O(n) CPU  0.020429  0.000070  0.020302  0.020571     144   \n",
       "52  torch_nl O(n) CPU  0.020426  0.000078  0.020236  0.020575     144   \n",
       "53  torch_nl O(n) CPU  0.029750  0.000132  0.029452  0.030134     216   \n",
       "\n",
       "    n_neighbor_per_atom_avg  \n",
       "0                        28  \n",
       "1                        28  \n",
       "2                        28  \n",
       "3                        28  \n",
       "4                        28  \n",
       "5                        28  \n",
       "6                        28  \n",
       "7                        28  \n",
       "8                        28  \n",
       "9                        28  \n",
       "10                       28  \n",
       "11                       28  \n",
       "12                       28  \n",
       "13                       28  \n",
       "14                       28  \n",
       "15                       28  \n",
       "16                       28  \n",
       "17                       28  \n",
       "18                       28  \n",
       "19                       28  \n",
       "20                       28  \n",
       "21                       28  \n",
       "22                       28  \n",
       "23                       28  \n",
       "24                       28  \n",
       "25                       28  \n",
       "26                       29  \n",
       "27                       28  \n",
       "28                       28  \n",
       "29                       28  \n",
       "30                       28  \n",
       "31                       28  \n",
       "32                       28  \n",
       "33                       28  \n",
       "34                       28  \n",
       "35                       28  \n",
       "36                       28  \n",
       "37                       28  \n",
       "38                       28  \n",
       "39                       28  \n",
       "40                       28  \n",
       "41                       28  \n",
       "42                       28  \n",
       "43                       28  \n",
       "44                       28  \n",
       "45                       28  \n",
       "46                       28  \n",
       "47                       28  \n",
       "48                       28  \n",
       "49                       28  \n",
       "50                       28  \n",
       "51                       28  \n",
       "52                       28  \n",
       "53                       29  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x7f1beec2dbb0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 484.25x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.lmplot(data=df, x='n_atom', y='mean', hue='tag',fit_reg=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2,\n",
       "         2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3]),\n",
       " tensor([ 6, 13,  8,  8]),\n",
       " torch.Size([35, 3]),\n",
       " torch.Size([12, 3]),\n",
       " torch.Size([12]))"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = [molecule('OCHCHO'), molecule('C3H9C')]\n",
    "# frames = [molecule('C3H9C')]\n",
    "frames += [bulk('Si', 'diamond', a=6, cubic=True)]\n",
    "frames += [bulk('Si', 'diamond', a=4, cubic=True)]\n",
    "# frames = [bulk('Si', 'diamond', a=6.1, cubic=True)]\n",
    "rcut = 3\n",
    "\n",
    "# pos = torch.from_numpy(frame.get_positions())\n",
    "# cell = torch.from_numpy(frame.get_cell().array)\n",
    "# pbc = torch.from_numpy(frame.get_pbc())\n",
    "n_atoms = [0]\n",
    "pos = []\n",
    "cell = []\n",
    "pbc = []\n",
    "for ff in frames:\n",
    "    n_atoms.append(len(ff))\n",
    "    pos.append(torch.from_numpy(ff.get_positions()))\n",
    "    cell.append(torch.from_numpy(ff.get_cell().array))\n",
    "    pbc.append(torch.from_numpy(ff.get_pbc()))\n",
    "pos = torch.cat(pos)\n",
    "cell = torch.cat(cell)\n",
    "pbc = torch.cat(pbc)\n",
    "stride = torch.from_numpy(np.cumsum(n_atoms))\n",
    "batch = torch.zeros(pos.shape[0],dtype=torch.long)\n",
    "for ii,(st,nd) in enumerate(zip(stride[:-1],stride[1:])):\n",
    "    batch[st:nd] = ii\n",
    "n_atoms = torch.Tensor(n_atoms[1:]).to(dtype=torch.long)\n",
    "batch,n_atoms, pos.shape, cell.shape, pbc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "dist = []\n",
    "mm = []\n",
    "for frame in frames:\n",
    "    idx_i, idx_j, idx_S, dd = neighbor_list(\n",
    "            \"ijSd\", frame, cutoff=rcut, self_interaction=False\n",
    "        )\n",
    "    dist.append(np.sort(dd))\n",
    "    mm.append((idx_i, idx_j))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "code_folding": [
     0,
     17,
     24
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_fully_connected_mapping(i_ids, shifts_idx) -> torch.Tensor:\n",
    "    n_atom = i_ids.shape[0]\n",
    "    n_atom2 = n_atom * n_atom\n",
    "    n_cell_image = shifts_idx.shape[0]\n",
    "    j_ids = torch.repeat_interleave(i_ids, n_cell_image)\n",
    "    mapping = torch.cartesian_prod(i_ids, j_ids)\n",
    "    shifts_idx = shifts_idx.repeat((n_atom2, 1))\n",
    "    \n",
    "    mask = torch.ones(mapping.shape[0], dtype=bool, device=i_ids.device)\n",
    "    ids = n_cell_image*torch.arange(n_atom, device=i_ids.device) \\\n",
    "                + torch.arange(0, mapping.shape[0], n_atom*n_cell_image, device=i_ids.device)\n",
    "    # print(n_atom*n_cell_image, ids)\n",
    "    mask[ids] = False\n",
    "    mapping = mapping[mask, :]\n",
    "    shifts_idx = shifts_idx[mask]\n",
    "    return mapping, shifts_idx\n",
    "\n",
    "def compute_images(\n",
    "    positions: torch.Tensor,\n",
    "    cell: torch.Tensor,\n",
    "    pbc: torch.Tensor,\n",
    "    cutoff: float,\n",
    "    batch: torch.Tensor,\n",
    "    n_atoms: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"TODO: add doc\"\"\"\n",
    "    cell = cell.view((-1, 3, 3)).to(torch.float32)\n",
    "    pbc = pbc.view((-1, 3))\n",
    "    dtype = cell.dtype\n",
    "    has_pbc = pbc.prod(dim=1, dtype=bool)\n",
    "    stride = torch.zeros(n_atoms.shape[0]+1,dtype=torch.long)\n",
    "    stride[1:] = torch.cumsum(n_atoms, dim=0,dtype=torch.long)\n",
    "    reciprocal_cell = torch.zeros_like(cell)\n",
    "    reciprocal_cell[has_pbc,:,:] = torch.linalg.inv(cell[has_pbc,:,:]).transpose(2, 1)\n",
    "    inv_distances = reciprocal_cell.norm(2, dim=-1)\n",
    "    num_repeats = torch.ceil(cutoff * inv_distances).to(torch.long)\n",
    "    num_repeats_ = torch.where(pbc, num_repeats, torch.zeros_like(num_repeats))\n",
    "    ids = torch.arange(positions.shape[0], device=positions.device, dtype=torch.long)\n",
    "    images, mapping, batch_mapping, shifts_expanded, shifts_idx_ = [], [], [], [], []\n",
    "    for i_structure in range(n_atoms.shape[0]):\n",
    "        num_repeats = num_repeats_[i_structure]\n",
    "        reps = []\n",
    "        for ii in range(3):\n",
    "            r1 = torch.arange(\n",
    "                -num_repeats[ii],\n",
    "                num_repeats[ii] + 1,\n",
    "                device=cell.device,\n",
    "                dtype=dtype,\n",
    "            )\n",
    "            _, indices = torch.sort(torch.abs(r1))\n",
    "            reps.append(r1[indices])\n",
    "        shifts_idx = torch.cartesian_prod(*reps)\n",
    "        n_cell_image = shifts_idx.shape[0]\n",
    "        n_atom = n_atoms[i_structure]\n",
    "        pos = positions[stride[i_structure]:stride[i_structure+1]]\n",
    "        i_ids = ids[stride[i_structure]:stride[i_structure+1]]\n",
    "        \n",
    "        s_mapping, shifts_idx = get_fully_connected_mapping(i_ids, shifts_idx)\n",
    "        mapping.append(s_mapping)\n",
    "        batch_mapping.append(\n",
    "            i_structure\n",
    "            * torch.ones(\n",
    "                s_mapping.shape[0], dtype=torch.long, device=cell.device\n",
    "            )\n",
    "        )\n",
    "        shifts_idx_.append(shifts_idx)\n",
    "    return (\n",
    "        torch.cat(mapping, dim=0).t(),\n",
    "        torch.cat(batch_mapping, dim=0),\n",
    "        torch.cat(shifts_idx_, dim=0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "def compute_distances(\n",
    "    pos: torch.Tensor,\n",
    "    mapping: torch.Tensor,\n",
    "    cell_shifts: Optional[torch.Tensor] = None,\n",
    "):\n",
    "    assert mapping.dim() == 2\n",
    "    assert mapping.shape[0] == 2\n",
    "\n",
    "    if cell_shifts is None:\n",
    "        dr = pos[mapping[1]] - pos[mapping[0]]\n",
    "    else:\n",
    "        dr = pos[mapping[1]] - pos[mapping[0]] + cell_shifts\n",
    "\n",
    "    return dr.norm(p=2, dim=1)\n",
    "\n",
    "def compute_cell_shifts(cell, shifts_idx, batch_mapping):\n",
    "    if cell is None:\n",
    "        cell_shifts = None\n",
    "    else:\n",
    "        cell_shifts = torch.einsum(\"jn,jnm->jm\", shifts_idx, cell.view(-1, 3, 3)[batch_mapping])\n",
    "    return cell_shifts\n",
    "\n",
    "def compute_strict_nl_n2(pos, cell, mapping, batch_mapping,  shifts_idx):\n",
    "    cell_shifts = compute_cell_shifts(cell, shifts_idx, batch_mapping)\n",
    "    d2 = (pos[mapping[0]] - pos[mapping[1]] - cell_shifts).square().sum(dim=1)\n",
    "    mask = d2 <= rcut*rcut\n",
    "    mapping = mapping[:, mask]\n",
    "    mapping_batch = batch_mapping[mask]\n",
    "    shifts_idx = shifts_idx[mask]\n",
    "    return mapping, mapping_batch, shifts_idx # , d2[mask].sqrt()\n",
    "\n",
    "def compute_nl(pos, cell, pbc, rcut, batch, method='n2'):\n",
    "    n_atoms = torch.bincount(batch)\n",
    "    mapping, batch_mapping, shifts_idx = compute_images(pos, \n",
    "                                     cell, pbc, rcut, \n",
    "                                     batch, \n",
    "                                     n_atoms)\n",
    "    if method == 'n2':\n",
    "        mapping, mapping_batch, shifts_idx = compute_strict_nl_n2(pos, cell, mapping, batch_mapping,  shifts_idx)\n",
    "    \n",
    "    return mapping, mapping_batch, shifts_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Float but found Double",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [222]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m mapping, batch_mapping, shifts_idx \u001b[38;5;241m=\u001b[39m compute_images(pos, \n\u001b[1;32m      2\u001b[0m                                      cell, pbc, rcut, \n\u001b[1;32m      3\u001b[0m                                      batch, \n\u001b[1;32m      4\u001b[0m                                      n_atoms)\n\u001b[0;32m----> 6\u001b[0m cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_cell_shifts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m d2 \u001b[38;5;241m=\u001b[39m (pos[mapping[\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m-\u001b[39m pos[mapping[\u001b[38;5;241m1\u001b[39m]] \u001b[38;5;241m-\u001b[39m cell_shifts )\u001b[38;5;241m.\u001b[39msquare()\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m mask \u001b[38;5;241m=\u001b[39m d2 \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m rcut\u001b[38;5;241m*\u001b[39mrcut\n",
      "Input \u001b[0;32mIn [221]\u001b[0m, in \u001b[0;36mcompute_cell_shifts\u001b[0;34m(cell, shifts_idx, batch_mapping)\u001b[0m\n\u001b[1;32m     19\u001b[0m     cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m     cell_shifts \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mjn,jnm->jm\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshifts_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_mapping\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cell_shifts\n",
      "File \u001b[0;32m/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/torch/functional.py:330\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[38;5;66;03m# recurse incase operands contains value that has torch function\u001b[39;00m\n\u001b[1;32m    327\u001b[0m     \u001b[38;5;66;03m# in the original implementation this line is omitted\u001b[39;00m\n\u001b[1;32m    328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[0;32m--> 330\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meinsum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mequation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperands\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Float but found Double"
     ]
    }
   ],
   "source": [
    "mapping, batch_mapping, shifts_idx = compute_images(pos, \n",
    "                                     cell, pbc, rcut, \n",
    "                                     batch, \n",
    "                                     n_atoms)\n",
    "\n",
    "cell_shifts = compute_cell_shifts(cell, shifts_idx, batch_mapping)\n",
    "d2 = (pos[mapping[0]] - pos[mapping[1]] - cell_shifts ).square().sum(dim=1)\n",
    "mask = d2 <= rcut*rcut\n",
    "d = d2[mask].sqrt()\n",
    "mapping = mapping[:, mask]\n",
    "mapping_batch = batch_mapping[mask]\n",
    "d.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((26,),\n",
       " array([1.10563938, 1.10563938, 1.10563938, 1.10563938, 1.22295086,\n",
       "        1.22295086, 1.22295086, 1.22295086, 1.51286   , 1.51286   ,\n",
       "        2.05241169, 2.05241169, 2.05241169, 2.05241169, 2.22229623,\n",
       "        2.22229623, 2.22229623, 2.22229623, 2.38769877, 2.38769877,\n",
       "        2.38769877, 2.38769877, 2.61851372, 2.61851372, 2.61851372,\n",
       "        2.61851372]))"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0].shape,dist[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([216, 3]),\n",
       " torch.Size([35, 3]),\n",
       " torch.Size([2, 288]),\n",
       " torch.Size([216, 3]),\n",
       " torch.Size([3626, 3]))"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, pos.shape, mapping.shape, shifts_expanded.shape, shifts_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "for ii, dd in enumerate(dist):\n",
    "    print(np.allclose(np.sort(dd), np.sort(d[mapping_batch == ii].numpy())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(1.1977, dtype=torch.float64), 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2.min(), rcut*rcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((32,), torch.Size([23]), torch.Size([1720]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist[0].shape, d.shape, d2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 23]),\n",
       " torch.Size([216, 3]),\n",
       " tensor([[ 0,  1,  1,  1,  1,  2,  2,  3,  3,  3,  3,  4,  4,  5,  5,  5,  5,  6,\n",
       "           6,  7,  7,  7,  7],\n",
       "         [ 1,  0,  2,  4,  6,  1,  3,  2, 14, 28, 32,  1,  5,  4, 14, 74, 80,  1,\n",
       "           7,  6, 28, 74, 96]]))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping.shape, images.shape, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0],\n",
       "        [1, 1],\n",
       "        [2, 2]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.arange(3).reshape(3,1)\n",
    "torch.cat([aa,aa], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "torch.arange?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 4, 4, 4, 4,\n",
       "         4, 5, 5, 5, 5, 5],\n",
       "        [1, 2, 3, 4, 5, 0, 2, 3, 4, 5, 0, 1, 3, 4, 5, 0, 1, 2, 4, 5, 0, 1, 2, 3,\n",
       "         5, 0, 1, 2, 3, 4]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# linked cell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_nl import (compute_nl_n2, strict_nl, compute_distances, compute_cell_shifts, \n",
    "                      compute_images, ase2data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_nl.image import get_number_of_cell_repeats, get_cell_shift_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stride(v):\n",
    "    v = v.flatten()\n",
    "    stride = v.new_empty(v.shape[0]+1)\n",
    "    stride[0] = 0\n",
    "    stride[1:] = torch.cumsum(v, dim=0, dtype=stride.dtype)\n",
    "    return stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = [molecule('OCHCHO'), molecule('C3H9C')]\n",
    "frames = [molecule('C3H9C')]\n",
    "frames += [bulk('Si', 'diamond', a=4, cubic=False)]\n",
    "frames += [bulk('Si', 'diamond', a=4, cubic=True)]\n",
    "frames += [\n",
    "    bulk('Si', 'diamond', a=6.1, cubic=True), \n",
    "    bulk('Si', 'diamond', a=3.1, cubic=True)\n",
    "]\n",
    "rcut = 3\n",
    "self_interaction = False\n",
    "pos, cell, pbc, batch, n_atoms = ase2data(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((518,),\n",
       " array([1.09439479, 1.09439479, 1.09439479, 1.09439479, 1.09439488,\n",
       "        1.09439488, 1.09439488, 1.09439488, 1.09439498, 1.09439498,\n",
       "        1.09439498, 1.09439498, 1.10242263, 1.10242263, 1.10242288,\n",
       "        1.10242288, 1.10242288, 1.10242288, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.49342443, 1.49342443, 1.49342443, 1.49342443, 1.4934251 ,\n",
       "        1.4934251 , 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.76549069, 1.76549069, 1.76549069, 1.76549069,\n",
       "        1.76549084, 1.76549084, 1.76549084, 1.76549084, 1.76549092,\n",
       "        1.76549092, 1.76549092, 1.76549092, 1.77408553, 1.77408553,\n",
       "        1.77408553, 1.77408553, 1.774086  , 1.774086  , 2.15029608,\n",
       "        2.15029608, 2.15029608, 2.15029608, 2.15029623, 2.15029623,\n",
       "        2.15029623, 2.15029623, 2.15029637, 2.15029637, 2.15029637,\n",
       "        2.15029637, 2.1566541 , 2.1566541 , 2.15665443, 2.15665443,\n",
       "        2.15665443, 2.15665443, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.483128  , 2.483128  ,\n",
       "        2.48312806, 2.48312806, 2.48312806, 2.48312806, 2.560294  ,\n",
       "        2.560294  , 2.56029431, 2.56029431, 2.56029431, 2.56029431,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.74867904, 2.74867904,\n",
       "        2.74867904, 2.74867904, 2.74867933, 2.74867933, 2.74867933,\n",
       "        2.74867933, 2.74867985, 2.74867985, 2.74867985, 2.74867985,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.98224019, 2.98224019, 2.98224019, 2.98224019, 2.98224082,\n",
       "        2.98224082, 2.98224082, 2.98224082, 2.98224106, 2.98224106,\n",
       "        2.98224106, 2.98224106, 2.99903928, 2.99903928, 2.99903928,\n",
       "        2.99903928, 2.99904   , 2.99904   ]))"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping, batch_mapping, shifts_idx = compute_images(\n",
    "    pos, cell, pbc, rcut, n_atoms, self_interaction\n",
    ")\n",
    "mapping_r, batch_mapping_r, shifts_idx_r = strict_nl(\n",
    "    rcut, pos, cell, mapping, batch_mapping, shifts_idx\n",
    ")\n",
    "cell_shifts_r = compute_cell_shifts(cell, shifts_idx_r, batch_mapping_r)\n",
    "d_r = compute_distances(pos, mapping_r, cell_shifts_r).numpy()\n",
    "d_r = np.sort(d_r)\n",
    "d_r.shape,d_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shifts_idx tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], dtype=torch.float64)\n",
      "i_ids tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "n_cell_image 1 torch.Size([13]) torch.Size([13, 3]) tensor(13)\n",
      "torch.Size([13, 3]) tensor([2.1286, 1.9458, 0.4176], dtype=torch.float64) tensor([-2.1286, -1.7411, -1.0938], dtype=torch.float64)\n",
      "torch.Size([13, 3]) tensor([2.1286, 1.9458, 0.4176], dtype=torch.float64) tensor([-2.1286, -1.7411, -1.0938], dtype=torch.float64)\n",
      "box_length tensor([4.2582, 3.6879, 1.5124], dtype=torch.float64) tensor([2, 2, 1]) tensor(4)\n",
      "tensor([[4.2582, 0.0000, 0.0000],\n",
      "        [0.0000, 3.6879, 0.0000],\n",
      "        [0.0000, 0.0000, 1.5124]], dtype=torch.float64)\n",
      "bin_index_i tensor([0, 1, 2, 0, 1, 1, 3, 2, 2, 2, 0, 0, 0])\n",
      "bin_index_j tensor([0, 1, 2, 0, 1, 1, 3, 2, 2, 2, 0, 0, 0])\n",
      "atom_i tensor([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3]) tensor([ 0,  3, 10, 11, 12,  1,  4,  5,  2,  7,  8,  9,  6])\n",
      "atom_j tensor([0, 0, 0, 0, 0, 1, 1, 1, 2, 2, 2, 2, 3]) tensor([ 0,  3, 10, 11, 12,  1,  4,  5,  2,  7,  8,  9,  6])\n",
      "n_atom_per_bin tensor([5, 3, 4, 1]) tensor([5, 3, 4, 1])\n",
      "i_bins_s tensor([[0, 0, 0],\n",
      "        [0, 1, 0],\n",
      "        [1, 0, 0],\n",
      "        [1, 1, 0]])\n",
      "shifts_idx tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0., -2.],\n",
      "        [ 0.,  0., -2.],\n",
      "        [ 0.,  0.,  2.],\n",
      "        [ 0.,  0.,  2.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1., -2.],\n",
      "        [ 0., -1., -2.],\n",
      "        [ 0., -1.,  2.],\n",
      "        [ 0., -1.,  2.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1., -2.],\n",
      "        [ 0.,  1., -2.],\n",
      "        [ 0.,  1.,  2.],\n",
      "        [ 0.,  1.,  2.],\n",
      "        [ 0., -2.,  0.],\n",
      "        [ 0., -2.,  0.],\n",
      "        [ 0., -2., -1.],\n",
      "        [ 0., -2., -1.],\n",
      "        [ 0., -2.,  1.],\n",
      "        [ 0., -2.,  1.],\n",
      "        [ 0., -2., -2.],\n",
      "        [ 0., -2., -2.],\n",
      "        [ 0., -2.,  2.],\n",
      "        [ 0., -2.,  2.],\n",
      "        [ 0.,  2.,  0.],\n",
      "        [ 0.,  2.,  0.],\n",
      "        [ 0.,  2., -1.],\n",
      "        [ 0.,  2., -1.],\n",
      "        [ 0.,  2.,  1.],\n",
      "        [ 0.,  2.,  1.],\n",
      "        [ 0.,  2., -2.],\n",
      "        [ 0.,  2., -2.],\n",
      "        [ 0.,  2.,  2.],\n",
      "        [ 0.,  2.,  2.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0., -2.],\n",
      "        [-1.,  0., -2.],\n",
      "        [-1.,  0.,  2.],\n",
      "        [-1.,  0.,  2.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1., -2.],\n",
      "        [-1., -1., -2.],\n",
      "        [-1., -1.,  2.],\n",
      "        [-1., -1.,  2.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1., -2.],\n",
      "        [-1.,  1., -2.],\n",
      "        [-1.,  1.,  2.],\n",
      "        [-1.,  1.,  2.],\n",
      "        [-1., -2.,  0.],\n",
      "        [-1., -2.,  0.],\n",
      "        [-1., -2., -1.],\n",
      "        [-1., -2., -1.],\n",
      "        [-1., -2.,  1.],\n",
      "        [-1., -2.,  1.],\n",
      "        [-1., -2., -2.],\n",
      "        [-1., -2., -2.],\n",
      "        [-1., -2.,  2.],\n",
      "        [-1., -2.,  2.],\n",
      "        [-1.,  2.,  0.],\n",
      "        [-1.,  2.,  0.],\n",
      "        [-1.,  2., -1.],\n",
      "        [-1.,  2., -1.],\n",
      "        [-1.,  2.,  1.],\n",
      "        [-1.,  2.,  1.],\n",
      "        [-1.,  2., -2.],\n",
      "        [-1.,  2., -2.],\n",
      "        [-1.,  2.,  2.],\n",
      "        [-1.,  2.,  2.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0., -2.],\n",
      "        [ 1.,  0., -2.],\n",
      "        [ 1.,  0.,  2.],\n",
      "        [ 1.,  0.,  2.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1., -2.],\n",
      "        [ 1., -1., -2.],\n",
      "        [ 1., -1.,  2.],\n",
      "        [ 1., -1.,  2.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1., -2.],\n",
      "        [ 1.,  1., -2.],\n",
      "        [ 1.,  1.,  2.],\n",
      "        [ 1.,  1.,  2.],\n",
      "        [ 1., -2.,  0.],\n",
      "        [ 1., -2.,  0.],\n",
      "        [ 1., -2., -1.],\n",
      "        [ 1., -2., -1.],\n",
      "        [ 1., -2.,  1.],\n",
      "        [ 1., -2.,  1.],\n",
      "        [ 1., -2., -2.],\n",
      "        [ 1., -2., -2.],\n",
      "        [ 1., -2.,  2.],\n",
      "        [ 1., -2.,  2.],\n",
      "        [ 1.,  2.,  0.],\n",
      "        [ 1.,  2.,  0.],\n",
      "        [ 1.,  2., -1.],\n",
      "        [ 1.,  2., -1.],\n",
      "        [ 1.,  2.,  1.],\n",
      "        [ 1.,  2.,  1.],\n",
      "        [ 1.,  2., -2.],\n",
      "        [ 1.,  2., -2.],\n",
      "        [ 1.,  2.,  2.],\n",
      "        [ 1.,  2.,  2.],\n",
      "        [-2.,  0.,  0.],\n",
      "        [-2.,  0.,  0.],\n",
      "        [-2.,  0., -1.],\n",
      "        [-2.,  0., -1.],\n",
      "        [-2.,  0.,  1.],\n",
      "        [-2.,  0.,  1.],\n",
      "        [-2.,  0., -2.],\n",
      "        [-2.,  0., -2.],\n",
      "        [-2.,  0.,  2.],\n",
      "        [-2.,  0.,  2.],\n",
      "        [-2., -1.,  0.],\n",
      "        [-2., -1.,  0.],\n",
      "        [-2., -1., -1.],\n",
      "        [-2., -1., -1.],\n",
      "        [-2., -1.,  1.],\n",
      "        [-2., -1.,  1.],\n",
      "        [-2., -1., -2.],\n",
      "        [-2., -1., -2.],\n",
      "        [-2., -1.,  2.],\n",
      "        [-2., -1.,  2.],\n",
      "        [-2.,  1.,  0.],\n",
      "        [-2.,  1.,  0.],\n",
      "        [-2.,  1., -1.],\n",
      "        [-2.,  1., -1.],\n",
      "        [-2.,  1.,  1.],\n",
      "        [-2.,  1.,  1.],\n",
      "        [-2.,  1., -2.],\n",
      "        [-2.,  1., -2.],\n",
      "        [-2.,  1.,  2.],\n",
      "        [-2.,  1.,  2.],\n",
      "        [-2., -2.,  0.],\n",
      "        [-2., -2.,  0.],\n",
      "        [-2., -2., -1.],\n",
      "        [-2., -2., -1.],\n",
      "        [-2., -2.,  1.],\n",
      "        [-2., -2.,  1.],\n",
      "        [-2., -2., -2.],\n",
      "        [-2., -2., -2.],\n",
      "        [-2., -2.,  2.],\n",
      "        [-2., -2.,  2.],\n",
      "        [-2.,  2.,  0.],\n",
      "        [-2.,  2.,  0.],\n",
      "        [-2.,  2., -1.],\n",
      "        [-2.,  2., -1.],\n",
      "        [-2.,  2.,  1.],\n",
      "        [-2.,  2.,  1.],\n",
      "        [-2.,  2., -2.],\n",
      "        [-2.,  2., -2.],\n",
      "        [-2.,  2.,  2.],\n",
      "        [-2.,  2.,  2.],\n",
      "        [ 2.,  0.,  0.],\n",
      "        [ 2.,  0.,  0.],\n",
      "        [ 2.,  0., -1.],\n",
      "        [ 2.,  0., -1.],\n",
      "        [ 2.,  0.,  1.],\n",
      "        [ 2.,  0.,  1.],\n",
      "        [ 2.,  0., -2.],\n",
      "        [ 2.,  0., -2.],\n",
      "        [ 2.,  0.,  2.],\n",
      "        [ 2.,  0.,  2.],\n",
      "        [ 2., -1.,  0.],\n",
      "        [ 2., -1.,  0.],\n",
      "        [ 2., -1., -1.],\n",
      "        [ 2., -1., -1.],\n",
      "        [ 2., -1.,  1.],\n",
      "        [ 2., -1.,  1.],\n",
      "        [ 2., -1., -2.],\n",
      "        [ 2., -1., -2.],\n",
      "        [ 2., -1.,  2.],\n",
      "        [ 2., -1.,  2.],\n",
      "        [ 2.,  1.,  0.],\n",
      "        [ 2.,  1.,  0.],\n",
      "        [ 2.,  1., -1.],\n",
      "        [ 2.,  1., -1.],\n",
      "        [ 2.,  1.,  1.],\n",
      "        [ 2.,  1.,  1.],\n",
      "        [ 2.,  1., -2.],\n",
      "        [ 2.,  1., -2.],\n",
      "        [ 2.,  1.,  2.],\n",
      "        [ 2.,  1.,  2.],\n",
      "        [ 2., -2.,  0.],\n",
      "        [ 2., -2.,  0.],\n",
      "        [ 2., -2., -1.],\n",
      "        [ 2., -2., -1.],\n",
      "        [ 2., -2.,  1.],\n",
      "        [ 2., -2.,  1.],\n",
      "        [ 2., -2., -2.],\n",
      "        [ 2., -2., -2.],\n",
      "        [ 2., -2.,  2.],\n",
      "        [ 2., -2.,  2.],\n",
      "        [ 2.,  2.,  0.],\n",
      "        [ 2.,  2.,  0.],\n",
      "        [ 2.,  2., -1.],\n",
      "        [ 2.,  2., -1.],\n",
      "        [ 2.,  2.,  1.],\n",
      "        [ 2.,  2.,  1.],\n",
      "        [ 2.,  2., -2.],\n",
      "        [ 2.,  2., -2.],\n",
      "        [ 2.,  2.,  2.],\n",
      "        [ 2.,  2.,  2.]], dtype=torch.float64)\n",
      "i_ids tensor([13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14,\n",
      "        13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14, 13, 14])\n",
      "n_cell_image 125 torch.Size([250]) torch.Size([250, 3]) tensor(2)\n",
      "torch.Size([2, 3]) tensor([1., 1., 1.], dtype=torch.float64) tensor([0., 0., 0.], dtype=torch.float64)\n",
      "torch.Size([250, 3]) tensor([9., 9., 9.], dtype=torch.float64) tensor([-8., -8., -8.], dtype=torch.float64)\n",
      "box_length tensor([17.0010, 17.0010, 17.0010], dtype=torch.float64) tensor([6, 6, 6]) tensor(216)\n",
      "tensor([[17.0010,  0.0000,  0.0000],\n",
      "        [ 0.0000, 17.0010,  0.0000],\n",
      "        [ 0.0000,  0.0000, 17.0010]], dtype=torch.float64)\n",
      "bin_index_i tensor([ 86, 129])\n",
      "bin_index_j tensor([ 86, 129,  86,  87, 128, 129,  44,  45, 170, 171,  86,  92,  50,  50,\n",
      "         92, 128,   8,  44, 134, 134, 123, 129,  87, 123, 165, 165,  81,  81,\n",
      "        171, 207,  49,  55,  13,  49,  91,  91,   7,   7,  97, 133, 160, 166,\n",
      "        124, 124, 166, 202,  82, 118, 208, 208,  86, 122,  80,  80, 122, 128,\n",
      "         38,  44, 164, 164,  85,  85,  43,  43,  85, 127,   1,  43, 127, 127,\n",
      "        122, 123,  80, 117, 158, 165,  74,  81, 164, 201,  48,  49,   6,  43,\n",
      "         84,  91,   0,   7,  90, 127, 159, 159, 117, 117, 159, 201,  75, 117,\n",
      "        201, 201,  93, 129,  87,  93, 135, 135,  51,  51, 171, 177,  92,  93,\n",
      "         50,  57,  98, 135,  14,  51, 134, 141, 130, 130,  88, 130, 172, 172,\n",
      "         88,  88, 172, 214,  56,  56,  14,  56,  98,  98,  14,  14,  98, 140,\n",
      "        166, 167, 124, 131, 172, 209,  88, 125, 208, 215,  79, 115,  73,  79,\n",
      "        121, 121,  37,  37, 157, 163,  78,  79,  36,  43,  84, 121,   0,  37,\n",
      "        120, 127, 116, 116,  74, 116, 158, 158,  74,  74, 158, 200,  42,  42,\n",
      "          0,  42,  84,  84,   0,   0,  84, 126, 152, 153, 110, 117, 158, 195,\n",
      "         74, 111, 194, 201, 100, 136,  94,  94, 136, 142,  52,  58, 178, 178,\n",
      "         99,  99,  57,  57,  99, 141,  15,  57, 141, 141, 136, 137,  94, 131,\n",
      "        172, 179,  88,  95, 178, 215,  62,  63,  20,  57,  98, 105,  14,  21,\n",
      "        104, 141, 173, 173, 131, 131, 173, 215,  89, 131, 215, 215])\n",
      "atom_i tensor([ 86, 129]) tensor([0, 1])\n",
      "atom_j tensor([  0,   0,   0,   0,   0,   1,   6,   7,   7,   7,   8,  13,  14,  14,\n",
      "         14,  14,  14,  15,  20,  21,  36,  37,  37,  37,  38,  42,  42,  42,\n",
      "         43,  43,  43,  43,  43,  44,  44,  44,  45,  48,  49,  49,  49,  50,\n",
      "         50,  50,  51,  51,  51,  52,  55,  56,  56,  56,  57,  57,  57,  57,\n",
      "         57,  58,  62,  63,  73,  74,  74,  74,  74,  74,  75,  78,  79,  79,\n",
      "         79,  80,  80,  80,  81,  81,  81,  82,  84,  84,  84,  84,  84,  85,\n",
      "         85,  85,  86,  86,  86,  86,  87,  87,  87,  88,  88,  88,  88,  88,\n",
      "         89,  90,  91,  91,  91,  92,  92,  92,  93,  93,  93,  94,  94,  94,\n",
      "         95,  97,  98,  98,  98,  98,  98,  99,  99,  99, 100, 104, 105, 110,\n",
      "        111, 115, 116, 116, 116, 117, 117, 117, 117, 117, 118, 120, 121, 121,\n",
      "        121, 122, 122, 122, 123, 123, 123, 124, 124, 124, 125, 126, 127, 127,\n",
      "        127, 127, 127, 128, 128, 128, 129, 129, 129, 129, 130, 130, 130, 131,\n",
      "        131, 131, 131, 131, 133, 134, 134, 134, 135, 135, 135, 136, 136, 136,\n",
      "        137, 140, 141, 141, 141, 141, 141, 142, 152, 153, 157, 158, 158, 158,\n",
      "        158, 158, 159, 159, 159, 160, 163, 164, 164, 164, 165, 165, 165, 166,\n",
      "        166, 166, 167, 170, 171, 171, 171, 172, 172, 172, 172, 172, 173, 173,\n",
      "        173, 177, 178, 178, 178, 179, 194, 195, 200, 201, 201, 201, 201, 201,\n",
      "        202, 207, 208, 208, 208, 209, 214, 215, 215, 215, 215, 215]) tensor([182, 187, 186, 166,  86,  66,  82,  36,  37,  87,  16,  32, 136, 137,\n",
      "        132, 236, 116, 216, 232, 237, 162, 156, 157, 167,  56, 180, 183, 181,\n",
      "         83,  62,  63, 163,  67,  57,  17,   6,   7,  80,  33,  81,  30, 112,\n",
      "         12,  13, 117, 106, 107, 206,  31, 133, 131, 130, 233, 113, 217, 213,\n",
      "        212, 207, 230, 231, 152,  76, 196, 177, 172, 176,  96, 160, 153, 150,\n",
      "        161,  72,  52,  53,  26,  27,  77,  46, 185, 184, 188,  84, 164,  60,\n",
      "         61,  64,   2,   0,  10,  50, 102,   3,  22, 127, 126, 226, 122, 146,\n",
      "        246,  88,  34,  35,  85, 110,  14,  11, 100, 111, 103, 203, 222, 202,\n",
      "        227,  38, 138, 234, 114, 135, 134, 210, 211, 214, 200, 238, 235, 192,\n",
      "        197, 151, 173, 170, 171,  97,  93,  92,  73, 193,  47, 168, 155, 154,\n",
      "        165,  54,  51,  70,  20,  23,  71,  43, 142,  42, 147, 189,  89,  69,\n",
      "         68, 169,  65,  55,   4,  15,   1,   5,  21, 101, 123, 121, 120, 242,\n",
      "        143, 243, 223, 247,  39, 118,  18,  19, 105, 104, 115, 220, 204, 201,\n",
      "        221, 139, 219, 239, 119, 215, 218, 205, 190, 191, 158, 174, 194, 175,\n",
      "        178,  74,  94,  91,  90,  40, 159,  59,  58,  78,  25,  24,  75,  44,\n",
      "         41, 140, 141,   8,  28, 108,   9, 224, 125, 124, 128, 144, 244, 241,\n",
      "        240, 109, 208, 228, 209, 225, 198, 195, 179, 199,  79,  99,  98,  95,\n",
      "         45,  29,  49,  48, 148, 145, 129, 248, 249, 245, 149, 229])\n",
      "n_atom_per_bin tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 1]) tensor([5, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 3, 5, 3, 1, 0, 0,\n",
      "        1, 3, 3, 3, 1, 0, 0, 1, 3, 5, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 5, 1, 0, 0, 1, 3, 3, 3, 1, 0, 5, 3, 4, 3, 5, 1, 1, 3, 3, 3, 3, 1,\n",
      "        0, 1, 5, 3, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 3, 5, 1, 0,\n",
      "        1, 3, 3, 3, 3, 1, 1, 5, 3, 4, 3, 5, 0, 1, 3, 3, 3, 1, 0, 0, 1, 5, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 5, 3, 1, 0, 0, 1, 3, 3, 3, 1,\n",
      "        0, 0, 1, 3, 5, 3, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 1, 1, 0, 0, 0, 0, 1, 5, 1, 0, 0, 0, 0, 1, 3, 1, 0, 0, 0, 0, 1, 5])\n",
      "i_bins_s tensor([[2, 2, 2],\n",
      "        [3, 3, 3]])\n",
      "shifts_idx tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]], dtype=torch.float64)\n",
      "i_ids tensor([15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "        15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22,\n",
      "        15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16,\n",
      "        17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18,\n",
      "        19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20,\n",
      "        21, 22, 15, 16, 17, 18, 19, 20, 21, 22, 15, 16, 17, 18, 19, 20, 21, 22])\n",
      "n_cell_image 27 torch.Size([216]) torch.Size([216, 3]) tensor(8)\n",
      "torch.Size([8, 3]) tensor([3., 3., 3.], dtype=torch.float64) tensor([0., 0., 0.], dtype=torch.float64)\n",
      "torch.Size([216, 3]) tensor([7., 7., 7.], dtype=torch.float64) tensor([-4., -4., -4.], dtype=torch.float64)\n",
      "box_length tensor([11.0010, 11.0010, 11.0010], dtype=torch.float64) tensor([4, 4, 4]) tensor(64)\n",
      "tensor([[11.0010,  0.0000,  0.0000],\n",
      "        [ 0.0000, 11.0010,  0.0000],\n",
      "        [ 0.0000,  0.0000, 11.0010]], dtype=torch.float64)\n",
      "bin_index_i tensor([21, 21, 26, 26, 38, 38, 41, 41])\n",
      "bin_index_j tensor([21, 21, 26, 26, 38, 38, 41, 41, 20, 20, 24, 25, 36, 37, 40, 40, 22, 23,\n",
      "        27, 27, 39, 39, 42, 43, 17, 17, 18, 22, 34, 34, 33, 37, 16, 16, 16, 21,\n",
      "        32, 33, 32, 36, 18, 19, 19, 23, 35, 35, 34, 39, 25, 29, 30, 30, 42, 46,\n",
      "        45, 45, 24, 28, 28, 29, 40, 45, 44, 44, 26, 31, 31, 31, 43, 47, 46, 47,\n",
      "         5,  5, 10, 10,  6, 22,  9, 25,  4,  4,  8,  9,  4, 21,  8, 24,  6,  7,\n",
      "        11, 11,  7, 23, 10, 27,  1,  1,  2,  6,  2, 18,  1, 21,  0,  0,  0,  5,\n",
      "         0, 17,  0, 20,  2,  3,  3,  7,  3, 19,  2, 23,  9, 13, 14, 14, 10, 30,\n",
      "        13, 29,  8, 12, 12, 13,  8, 29, 12, 28, 10, 15, 15, 15, 11, 31, 14, 31,\n",
      "        37, 53, 42, 58, 54, 54, 57, 57, 36, 52, 40, 57, 52, 53, 56, 56, 38, 55,\n",
      "        43, 59, 55, 55, 58, 59, 33, 49, 34, 54, 50, 50, 49, 53, 32, 48, 32, 53,\n",
      "        48, 49, 48, 52, 34, 51, 35, 55, 51, 51, 50, 55, 41, 61, 46, 62, 58, 62,\n",
      "        61, 61, 40, 60, 44, 61, 56, 61, 60, 60, 42, 63, 47, 63, 59, 63, 62, 63])\n",
      "atom_i tensor([21, 21, 26, 26, 38, 38, 41, 41]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "atom_j tensor([ 0,  0,  0,  0,  0,  1,  1,  1,  2,  2,  2,  2,  3,  3,  3,  4,  4,  4,\n",
      "         5,  5,  5,  6,  6,  6,  7,  7,  7,  8,  8,  8,  8,  9,  9,  9, 10, 10,\n",
      "        10, 10, 10, 11, 11, 11, 12, 12, 12, 13, 13, 13, 14, 14, 14, 15, 15, 15,\n",
      "        16, 16, 16, 17, 17, 17, 18, 18, 18, 19, 19, 19, 20, 20, 20, 21, 21, 21,\n",
      "        21, 21, 22, 22, 22, 23, 23, 23, 23, 24, 24, 24, 25, 25, 25, 26, 26, 26,\n",
      "        27, 27, 27, 28, 28, 28, 29, 29, 29, 29, 30, 30, 30, 31, 31, 31, 31, 31,\n",
      "        32, 32, 32, 32, 33, 33, 33, 34, 34, 34, 34, 34, 35, 35, 35, 36, 36, 36,\n",
      "        37, 37, 37, 38, 38, 38, 39, 39, 39, 40, 40, 40, 40, 40, 41, 41, 41, 42,\n",
      "        42, 42, 42, 43, 43, 43, 44, 44, 44, 45, 45, 45, 46, 46, 46, 47, 47, 47,\n",
      "        48, 48, 48, 49, 49, 49, 50, 50, 50, 51, 51, 51, 52, 52, 52, 53, 53, 53,\n",
      "        53, 54, 54, 54, 55, 55, 55, 55, 55, 56, 56, 56, 57, 57, 57, 58, 58, 58,\n",
      "        59, 59, 59, 60, 60, 60, 61, 61, 61, 61, 61, 62, 62, 62, 63, 63, 63, 63]) tensor([110, 104, 105, 106, 108, 102,  97,  96, 118, 112,  98, 100, 113, 114,\n",
      "        116,  84,  80,  81, 107,  72,  73,  76,  99,  88, 115,  92,  89,  82,\n",
      "         86, 128, 132, 120,  78,  83, 124,  94, 136,  75,  74,  90,  91, 140,\n",
      "        130, 129, 134, 121, 131, 126, 142, 123, 122, 139, 138, 137,  34,  33,\n",
      "         32, 109,  24,  25, 101,  26,  40, 117,  41,  42, 111,   9,   8,  85,\n",
      "          1, 103,  35,   0,  27,  77,  16, 119,  43,  17,  93,  10,  56,  87,\n",
      "         11,  48,  79,   3,  64,   2,  95,  19,  18,  58,  57, 135,  59, 133,\n",
      "        127,  49, 125,  50,  51,  65, 141,  66,  67, 143, 176,  38,  36, 178,\n",
      "        168,  30,  37,  28, 170, 184,  46,  29,  45, 186,  44,  39, 152,  12,\n",
      "        144,  31,  13,   5,   4, 160,  20,  21,  47,  60,  14, 154, 200,  15,\n",
      "          6,   7, 192, 208,  22,  52, 146,  68, 162,  23,  62,  63, 202,  61,\n",
      "         55,  54,  70, 194,  53, 210,  71,  69, 182, 180, 177, 174, 181, 169,\n",
      "        190, 172, 173, 189, 188, 185, 153, 183, 156, 179, 157, 145, 175, 149,\n",
      "        148, 171, 191, 161, 164, 187, 165, 204, 159, 158, 151, 155, 150, 147,\n",
      "        196, 166, 212, 163, 167, 207, 206, 201, 203, 205, 199, 198, 193, 197,\n",
      "        195, 214, 209, 211, 213, 215])\n",
      "n_atom_per_bin tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 2]) tensor([5, 3, 4, 3, 3, 3, 3, 3, 4, 3, 5, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 3, 4,\n",
      "        3, 3, 3, 3, 3, 4, 3, 5, 4, 3, 5, 3, 3, 3, 3, 3, 5, 3, 4, 3, 3, 3, 3, 3,\n",
      "        3, 3, 3, 3, 3, 4, 3, 5, 3, 3, 3, 3, 3, 5, 3, 4])\n",
      "i_bins_s tensor([[1, 1, 1],\n",
      "        [1, 2, 2],\n",
      "        [2, 1, 2],\n",
      "        [2, 2, 1]])\n",
      "shifts_idx tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]], dtype=torch.float64)\n",
      "i_ids tensor([23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24,\n",
      "        25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26,\n",
      "        27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28,\n",
      "        29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30,\n",
      "        23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24,\n",
      "        25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26,\n",
      "        27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28,\n",
      "        29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30,\n",
      "        23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24,\n",
      "        25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26,\n",
      "        27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28,\n",
      "        29, 30, 23, 24, 25, 26, 27, 28, 29, 30, 23, 24, 25, 26, 27, 28, 29, 30])\n",
      "n_cell_image 27 torch.Size([216]) torch.Size([216, 3]) tensor(8)\n",
      "torch.Size([8, 3]) tensor([4.5750, 4.5750, 4.5750], dtype=torch.float64) tensor([0., 0., 0.], dtype=torch.float64)\n",
      "torch.Size([216, 3]) tensor([10.6750, 10.6750, 10.6750], dtype=torch.float64) tensor([-6.1000, -6.1000, -6.1000], dtype=torch.float64)\n",
      "box_length tensor([16.7760, 16.7760, 16.7760], dtype=torch.float64) tensor([6, 6, 6]) tensor(216)\n",
      "tensor([[16.7760,  0.0000,  0.0000],\n",
      "        [ 0.0000, 16.7760,  0.0000],\n",
      "        [ 0.0000,  0.0000, 16.7760]], dtype=torch.float64)\n",
      "bin_index_i tensor([ 86,  86,  93,  93, 123, 123, 128, 128])\n",
      "bin_index_j tensor([ 86,  86,  93,  93, 123, 123, 128, 128,  84,  84,  91,  91, 121, 121,\n",
      "        126, 126,  88,  88,  95,  95, 125, 125, 130, 130,  74,  74,  81,  81,\n",
      "        111, 111, 116, 116,  72,  72,  79,  79, 109, 109, 114, 114,  76,  76,\n",
      "         83,  83, 113, 113, 118, 118,  98,  98, 105, 105, 135, 135, 140, 140,\n",
      "         96,  96, 103, 103, 133, 133, 138, 138, 100, 100, 107, 107, 137, 137,\n",
      "        142, 142,  14,  14,  21,  21,  51,  51,  56,  56,  12,  12,  19,  19,\n",
      "         49,  49,  54,  54,  16,  16,  23,  23,  53,  53,  58,  58,   2,   2,\n",
      "          9,   9,  39,  39,  44,  44,   0,   0,   7,   7,  37,  37,  42,  42,\n",
      "          4,   4,  11,  11,  41,  41,  46,  46,  26,  26,  33,  33,  63,  63,\n",
      "         68,  68,  24,  24,  31,  31,  61,  61,  66,  66,  28,  28,  35,  35,\n",
      "         65,  65,  70,  70, 158, 158, 165, 165, 195, 195, 200, 200, 156, 156,\n",
      "        163, 163, 193, 193, 198, 198, 160, 160, 167, 167, 197, 197, 202, 202,\n",
      "        146, 146, 153, 153, 183, 183, 188, 188, 144, 144, 151, 151, 181, 181,\n",
      "        186, 186, 148, 148, 155, 155, 185, 185, 190, 190, 170, 170, 177, 177,\n",
      "        207, 207, 212, 212, 168, 168, 175, 175, 205, 205, 210, 210, 172, 172,\n",
      "        179, 179, 209, 209, 214, 214])\n",
      "atom_i tensor([ 86,  86,  93,  93, 123, 123, 128, 128]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "atom_j tensor([  0,   0,   2,   2,   4,   4,   7,   7,   9,   9,  11,  11,  12,  12,\n",
      "         14,  14,  16,  16,  19,  19,  21,  21,  23,  23,  24,  24,  26,  26,\n",
      "         28,  28,  31,  31,  33,  33,  35,  35,  37,  37,  39,  39,  41,  41,\n",
      "         42,  42,  44,  44,  46,  46,  49,  49,  51,  51,  53,  53,  54,  54,\n",
      "         56,  56,  58,  58,  61,  61,  63,  63,  65,  65,  66,  66,  68,  68,\n",
      "         70,  70,  72,  72,  74,  74,  76,  76,  79,  79,  81,  81,  83,  83,\n",
      "         84,  84,  86,  86,  88,  88,  91,  91,  93,  93,  95,  95,  96,  96,\n",
      "         98,  98, 100, 100, 103, 103, 105, 105, 107, 107, 109, 109, 111, 111,\n",
      "        113, 113, 114, 114, 116, 116, 118, 118, 121, 121, 123, 123, 125, 125,\n",
      "        126, 126, 128, 128, 130, 130, 133, 133, 135, 135, 137, 137, 138, 138,\n",
      "        140, 140, 142, 142, 144, 144, 146, 146, 148, 148, 151, 151, 153, 153,\n",
      "        155, 155, 156, 156, 158, 158, 160, 160, 163, 163, 165, 165, 167, 167,\n",
      "        168, 168, 170, 170, 172, 172, 175, 175, 177, 177, 179, 179, 181, 181,\n",
      "        183, 183, 185, 185, 186, 186, 188, 188, 190, 190, 193, 193, 195, 195,\n",
      "        197, 197, 198, 198, 200, 200, 202, 202, 205, 205, 207, 207, 209, 209,\n",
      "        210, 210, 212, 212, 214, 214]) tensor([104, 105,  96,  97, 113, 112, 106, 107,  98,  99, 114, 115,  81,  80,\n",
      "         72,  73,  88,  89,  83,  82,  74,  75,  90,  91, 129, 128, 121, 120,\n",
      "        137, 136, 131, 130, 123, 122, 139, 138, 108, 109, 100, 101, 117, 116,\n",
      "        110, 111, 103, 102, 119, 118,  85,  84,  76,  77,  92,  93,  86,  87,\n",
      "         78,  79,  95,  94, 133, 132, 125, 124, 140, 141, 135, 134, 127, 126,\n",
      "        143, 142,  33,  32,  25,  24,  41,  40,  35,  34,  27,  26,  43,  42,\n",
      "          9,   8,   1,   0,  17,  16,  10,  11,   2,   3,  19,  18,  57,  56,\n",
      "         48,  49,  65,  64,  59,  58,  51,  50,  67,  66,  36,  37,  29,  28,\n",
      "         45,  44,  39,  38,  31,  30,  46,  47,  13,  12,   4,   5,  21,  20,\n",
      "         14,  15,   7,   6,  22,  23,  61,  60,  53,  52,  68,  69,  62,  63,\n",
      "         55,  54,  70,  71, 177, 176, 169, 168, 185, 184, 179, 178, 171, 170,\n",
      "        187, 186, 152, 153, 144, 145, 160, 161, 154, 155, 146, 147, 162, 163,\n",
      "        200, 201, 192, 193, 209, 208, 202, 203, 195, 194, 211, 210, 180, 181,\n",
      "        173, 172, 188, 189, 183, 182, 175, 174, 190, 191, 157, 156, 148, 149,\n",
      "        165, 164, 158, 159, 151, 150, 167, 166, 204, 205, 197, 196, 212, 213,\n",
      "        206, 207, 199, 198, 214, 215])\n",
      "n_atom_per_bin tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 2, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 2, 0, 0, 0, 0, 2]) tensor([2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2,\n",
      "        2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
      "        0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
      "        2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2,\n",
      "        2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
      "        0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
      "        2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2,\n",
      "        2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0,\n",
      "        0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2, 0, 0, 2, 0, 2, 0, 2, 2, 0, 2, 0, 2])\n",
      "i_bins_s tensor([[2, 2, 2],\n",
      "        [2, 3, 3],\n",
      "        [3, 2, 3],\n",
      "        [3, 3, 2]])\n",
      "shifts_idx tensor([[ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0.,  0.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0., -1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0.,  0.,  1.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1.,  0.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1., -1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0., -1.,  1.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1.,  0.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1., -1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [ 0.,  1.,  1.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0.,  0.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0., -1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1.,  0.,  1.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1.,  0.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1., -1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1., -1.,  1.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1.,  0.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1., -1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [-1.,  1.,  1.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0.,  0.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0., -1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1.,  0.,  1.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1.,  0.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1., -1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1., -1.,  1.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1.,  0.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1., -1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.],\n",
      "        [ 1.,  1.,  1.]], dtype=torch.float64)\n",
      "i_ids tensor([31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32,\n",
      "        33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34,\n",
      "        35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36,\n",
      "        37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38,\n",
      "        31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32,\n",
      "        33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34,\n",
      "        35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36,\n",
      "        37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38,\n",
      "        31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32,\n",
      "        33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34,\n",
      "        35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36,\n",
      "        37, 38, 31, 32, 33, 34, 35, 36, 37, 38, 31, 32, 33, 34, 35, 36, 37, 38])\n",
      "n_cell_image 27 torch.Size([216]) torch.Size([216, 3]) tensor(8)\n",
      "torch.Size([8, 3]) tensor([2.3250, 2.3250, 2.3250], dtype=torch.float64) tensor([0., 0., 0.], dtype=torch.float64)\n",
      "torch.Size([216, 3]) tensor([5.4250, 5.4250, 5.4250], dtype=torch.float64) tensor([-3.1000, -3.1000, -3.1000], dtype=torch.float64)\n",
      "box_length tensor([8.5260, 8.5260, 8.5260], dtype=torch.float64) tensor([3, 3, 3]) tensor(27)\n",
      "tensor([[8.5260, 0.0000, 0.0000],\n",
      "        [0.0000, 8.5260, 0.0000],\n",
      "        [0.0000, 0.0000, 8.5260]], dtype=torch.float64)\n",
      "bin_index_i tensor([13, 13, 13, 13, 13, 13, 13, 13])\n",
      "bin_index_j tensor([13, 13, 13, 13, 13, 13, 13, 13, 12, 12, 12, 12, 12, 12, 12, 12, 14, 14,\n",
      "        14, 14, 14, 14, 14, 14, 10, 10, 10, 10, 10, 10, 10, 10,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9, 11, 11, 11, 11, 11, 11, 11, 11, 16, 16, 16, 16, 16, 16,\n",
      "        16, 16, 15, 15, 15, 15, 15, 15, 15, 15, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "         4,  4,  4,  4,  4,  4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  1,  1,  1,  1,  1,  1,  1,  1,  0,  0,  0,  0,\n",
      "         0,  0,  0,  0,  2,  2,  2,  2,  2,  2,  2,  2,  7,  7,  7,  7,  7,  7,\n",
      "         7,  7,  6,  6,  6,  6,  6,  6,  6,  6,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "        22, 22, 22, 22, 22, 22, 22, 22, 21, 21, 21, 21, 21, 21, 21, 21, 23, 23,\n",
      "        23, 23, 23, 23, 23, 23, 19, 19, 19, 19, 19, 19, 19, 19, 18, 18, 18, 18,\n",
      "        18, 18, 18, 18, 20, 20, 20, 20, 20, 20, 20, 20, 25, 25, 25, 25, 25, 25,\n",
      "        25, 25, 24, 24, 24, 24, 24, 24, 24, 24, 26, 26, 26, 26, 26, 26, 26, 26])\n",
      "atom_i tensor([13, 13, 13, 13, 13, 13, 13, 13]) tensor([0, 1, 2, 3, 4, 5, 6, 7])\n",
      "atom_j tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1,  1,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  3,  3,  3,  3,  3,  3,  3,  3,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  5,  5,  5,  5,  5,  5,  5,  5,  6,  6,  6,  6,  6,  6,\n",
      "         6,  6,  7,  7,  7,  7,  7,  7,  7,  7,  8,  8,  8,  8,  8,  8,  8,  8,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9, 10, 10, 10, 10, 10, 10, 10, 10, 11, 11,\n",
      "        11, 11, 11, 11, 11, 11, 12, 12, 12, 12, 12, 12, 12, 12, 13, 13, 13, 13,\n",
      "        13, 13, 13, 13, 14, 14, 14, 14, 14, 14, 14, 14, 15, 15, 15, 15, 15, 15,\n",
      "        15, 15, 16, 16, 16, 16, 16, 16, 16, 16, 17, 17, 17, 17, 17, 17, 17, 17,\n",
      "        18, 18, 18, 18, 18, 18, 18, 18, 19, 19, 19, 19, 19, 19, 19, 19, 20, 20,\n",
      "        20, 20, 20, 20, 20, 20, 21, 21, 21, 21, 21, 21, 21, 21, 22, 22, 22, 22,\n",
      "        22, 22, 22, 22, 23, 23, 23, 23, 23, 23, 23, 23, 24, 24, 24, 24, 24, 24,\n",
      "        24, 24, 25, 25, 25, 25, 25, 25, 25, 25, 26, 26, 26, 26, 26, 26, 26, 26]) tensor([104, 105, 106, 107, 108, 109, 110, 111,  97,  96,  98,  99, 100, 101,\n",
      "        102, 103, 118, 117, 116, 115, 114, 113, 112, 119,  87,  86,  85,  84,\n",
      "         83,  82,  81,  80,  72,  73,  74,  75,  77,  78,  79,  76,  88,  95,\n",
      "         94,  93,  92,  91,  90,  89, 134, 128, 135, 133, 132, 131, 130, 129,\n",
      "        124, 121, 122, 123, 120, 125, 126, 127, 143, 142, 141, 140, 139, 138,\n",
      "        137, 136,  39,  38,  37,  36,  35,  34,  33,  32,  24,  25,  26,  27,\n",
      "         29,  30,  31,  28,  40,  41,  47,  46,  45,  44,  43,  42,  11,   8,\n",
      "          9,  10,  12,  13,  14,  15,   1,   7,   0,   2,   3,   4,   5,   6,\n",
      "         23,  22,  21,  20,  19,  18,  17,  16,  56,  57,  58,  59,  60,  61,\n",
      "         62,  63,  55,  48,  49,  50,  51,  52,  53,  54,  70,  71,  65,  66,\n",
      "         67,  68,  69,  64, 176, 177, 178, 179, 180, 181, 182, 183, 175, 174,\n",
      "        173, 172, 171, 170, 169, 168, 184, 185, 186, 187, 189, 190, 191, 188,\n",
      "        153, 152, 154, 155, 156, 157, 158, 159, 151, 150, 149, 148, 147, 146,\n",
      "        145, 144, 160, 166, 165, 164, 163, 162, 161, 167, 200, 207, 206, 205,\n",
      "        204, 203, 202, 201, 199, 198, 197, 196, 195, 194, 193, 192, 208, 209,\n",
      "        210, 211, 212, 213, 214, 215])\n",
      "n_atom_per_bin tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8]) tensor([8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8, 8,\n",
      "        8, 8, 8])\n",
      "i_bins_s tensor([[1, 1, 1]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 0. Expected size 156 but got size 168 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [449]\u001b[0m, in \u001b[0;36m<cell line: 130>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    124\u001b[0m         cell_shifts_idx\u001b[38;5;241m.\u001b[39mappend(neigh_shift_idx)\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    126\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(mapping, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    127\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(batch_mapping, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    128\u001b[0m         torch\u001b[38;5;241m.\u001b[39mcat(cell_shifts_idx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    129\u001b[0m     )\n\u001b[0;32m--> 130\u001b[0m mapping, batch_mapping, shifts_idx \u001b[38;5;241m=\u001b[39m \u001b[43mlinked_cell\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_atoms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    131\u001b[0m mapping\u001b[38;5;241m.\u001b[39mshape, shifts_idx\u001b[38;5;241m.\u001b[39mshape,batch_mapping\u001b[38;5;241m.\u001b[39mshape\n\u001b[1;32m    133\u001b[0m mapping[:,\u001b[38;5;241m25\u001b[39m:\u001b[38;5;241m50\u001b[39m]\n",
      "Input \u001b[0;32mIn [449]\u001b[0m, in \u001b[0;36mlinked_cell\u001b[0;34m(pos, cell, pbc, rcut, n_atoms, self_interaction)\u001b[0m\n\u001b[1;32m    120\u001b[0m     batch_mapping\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m    121\u001b[0m         i_structure\n\u001b[1;32m    122\u001b[0m         \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(neigh_atom\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    123\u001b[0m     )\n\u001b[1;32m    124\u001b[0m     cell_shifts_idx\u001b[38;5;241m.\u001b[39mappend(neigh_shift_idx)\n\u001b[1;32m    125\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 126\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    127\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(batch_mapping, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    128\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcat(cell_shifts_idx, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[1;32m    129\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 0. Expected size 156 but got size 168 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def ravel_3d(idx_3d, shape):\n",
    "    idx_linear = (idx_3d[:, 2] + shape[2] * (idx_3d[:, 1] + shape[1] * idx_3d[:, 0]))\n",
    "    return idx_linear\n",
    "\n",
    "def unravel_3d(idx_linear, shape):\n",
    "    idx_3d = idx_linear.new_empty((idx_linear.shape[0], 3))\n",
    "    idx_3d[:, 2] = torch.remainder(idx_linear, shape[2]) \n",
    "    idx_3d[:, 1] = torch.remainder(torch.div(idx_linear, shape[2], rounding_mode=\"floor\"), shape[1])\n",
    "    idx_3d[:, 0] = torch.div(idx_linear, shape[1]*shape[2], rounding_mode=\"floor\") \n",
    "    return idx_3d\n",
    "\n",
    "\n",
    "def get_linear_bin_idx(cell, pos, nbins_s):\n",
    "    scaled_pos = torch.linalg.solve(cell.t(), pos.t()).t()\n",
    "    bin_index_s = torch.floor(scaled_pos * nbins_s).to(torch.long)\n",
    "    bin_index_l = ravel_3d(bin_index_s, nbins_s)\n",
    "    return bin_index_l\n",
    "\n",
    "def linked_cell(pos, cell, pbc, rcut, n_atoms, self_interaction=True):\n",
    "    n_structure = n_atoms.shape[0]\n",
    "    device = pos.device\n",
    "    dtype = pos.dtype\n",
    "    cell = cell.view((-1, 3, 3))\n",
    "    pbc = pbc.view((-1, 3))\n",
    "    \n",
    "    num_repeats_ = get_number_of_cell_repeats(rcut, cell, pbc)\n",
    "\n",
    "    stride = get_stride(n_atoms)\n",
    "    ids = torch.arange(pos.shape[0], device=device, dtype=torch.long)\n",
    "\n",
    "    mapping, batch_mapping, cell_shifts_idx = [], [], []\n",
    "    for i_structure in range(n_structure):\n",
    "        num_repeats = num_repeats_[i_structure]\n",
    "        n_atom = n_atoms[i_structure]\n",
    "        shifts_idx = get_cell_shift_idx(num_repeats, device, dtype)\n",
    "        n_cell_image = shifts_idx.shape[0]\n",
    "        shifts_idx = torch.repeat_interleave(shifts_idx, n_atom, dim=0)\n",
    "        i_ids = ids[stride[i_structure] : stride[i_structure + 1]]\n",
    "        i_pos = pos[i_ids]\n",
    "        \n",
    "        i_ids = i_ids.repeat(n_cell_image)\n",
    "        batch_image = torch.zeros((shifts_idx.shape[0]), dtype=torch.long)\n",
    "        cell_shifts = compute_cell_shifts(cell[i_structure], shifts_idx, batch_image)\n",
    "        images = pos[i_ids] + cell_shifts\n",
    "        \n",
    "        i_min = images.min(dim=0).values\n",
    "        images -= i_min - 1e-5\n",
    "        i_pos -= i_min - 1e-5\n",
    "        \n",
    "        box_length = images.max(dim=0).values - images.min(dim=0).values + 1e-3\n",
    "        nbins_s = torch.maximum(torch.ceil(box_length / rcut), pos.new_ones(3)).to(torch.long)\n",
    "        nbins = torch.prod(nbins_s)\n",
    "        box_vec = torch.diag_embed(box_length)\n",
    "        bin_index_i = get_linear_bin_idx(box_vec, i_pos, nbins_s)\n",
    "        bin_index_j = get_linear_bin_idx(box_vec, images, nbins_s)\n",
    "        sb_idx_i, atom_i = torch.sort(bin_index_i)\n",
    "        sb_idx_j, atom_j = torch.sort(bin_index_j)\n",
    "        n_atom_i_per_bin = torch.bincount(bin_index_i)\n",
    "        n_atom_j_per_bin = torch.bincount(sb_idx_j)\n",
    "        s_atom_j_bin_stride = get_stride(n_atom_j_per_bin)\n",
    "        s_atom_i_bin_stride = get_stride(n_atom_i_per_bin)\n",
    "        \n",
    "        i_bins_l = torch.unique(bin_index_i)\n",
    "        i_bins_s = unravel_3d(i_bins_l, nbins_s)\n",
    "        dd = torch.tensor([0,1,-1], dtype=torch.long)\n",
    "        bin_shifts = torch.cartesian_prod(dd,dd,dd).repeat((i_bins_s.shape[0], 1))\n",
    "        neigh_bins_s = (torch.repeat_interleave(i_bins_s, 27, dim=0) + bin_shifts)\n",
    "        neigh_bins_l = ravel_3d(neigh_bins_s, nbins_s)\n",
    "        mask = torch.logical_and(neigh_bins_l>=0, neigh_bins_l<nbins)\n",
    "        neigh_i_bins_l = torch.repeat_interleave(i_bins_l, 27, dim=0)[mask]\n",
    "        neigh_j_bins_l = neigh_bins_l[mask]\n",
    "        neigh_bins_l = torch.cat([neigh_i_bins_l.view(1,-1), neigh_j_bins_l.view(1,-1)], dim=0)\n",
    "        neigh_bins_l = torch.unique(neigh_bins_l, dim=1)\n",
    "        neigh_atom = []\n",
    "        for ii in range(neigh_bins_l.shape[1]):\n",
    "            i_bin, j_bin = neigh_bins_l[:,ii]\n",
    "            st,nd = s_atom_i_bin_stride[i_bin], s_atom_i_bin_stride[i_bin+1]\n",
    "            i_atoms = atom_i[st:nd]\n",
    "            \n",
    "            st,nd = s_atom_j_bin_stride[j_bin], s_atom_j_bin_stride[j_bin+1]\n",
    "            j_atoms = atom_j[st:nd]\n",
    "            neigh_atom.append(torch.cartesian_prod(i_atoms, j_atoms).t())\n",
    "            \n",
    "        neigh_atom = torch.cat(neigh_atom, dim=1)\n",
    "        if not self_interaction:\n",
    "            neigh_atom = neigh_atom[:, neigh_atom[0]!=neigh_atom[1]]\n",
    "        sorted_ids = torch.argsort(neigh_atom[0])\n",
    "        neigh_atom = neigh_atom[:, sorted_ids]\n",
    "        \n",
    "        neigh_shift_idx = shifts_idx[neigh_atom[1]]\n",
    "        neigh_atom[1] = torch.remainder(neigh_atom[1], n_atom)\n",
    "        mapping.append(neigh_atom + stride[i_structure])\n",
    "        batch_mapping.append(\n",
    "            i_structure\n",
    "            * torch.ones(neigh_atom.shape[1], dtype=torch.long, device=device)\n",
    "        )\n",
    "        cell_shifts_idx.append(neigh_shift_idx)\n",
    "    return (\n",
    "        torch.cat(mapping, dim=0),\n",
    "        torch.cat(batch_mapping, dim=0),\n",
    "        torch.cat(cell_shifts_idx, dim=0),\n",
    "    )\n",
    "mapping, batch_mapping, shifts_idx = linked_cell(pos, cell, pbc, rcut, n_atoms)\n",
    "mapping.shape, shifts_idx.shape,batch_mapping.shape\n",
    "\n",
    "mapping[:,25:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of torch_nl.neighbor_list failed: Traceback (most recent call last):\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/musil/git/torch_nl/torch_nl/neighbor_list.py\", line 4, in <module>\n",
      "    from .naive_impl import build_naive_neighborhood\n",
      "  File \"/home/musil/git/torch_nl/torch_nl/naive_impl.py\", line 4, in <module>\n",
      "    from .utils import get_number_of_cell_repeats, get_cell_shift_idx, strides_of\n",
      "ImportError: cannot import name 'get_number_of_cell_repeats' from 'torch_nl.utils' (/home/musil/git/torch_nl/torch_nl/utils.py)\n",
      "]\n",
      "[autoreload of torch_nl failed: Traceback (most recent call last):\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 257, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 455, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/local_scratch/musil/miniconda/envs/cg39/lib/python3.9/importlib/__init__.py\", line 169, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 613, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 850, in exec_module\n",
      "  File \"<frozen importlib._bootstrap>\", line 228, in _call_with_frames_removed\n",
      "  File \"/home/musil/git/torch_nl/torch_nl/__init__.py\", line 3, in <module>\n",
      "    from .neighbor_list import (\n",
      "ImportError: cannot import name 'compute_neighborlist' from 'torch_nl.neighbor_list' (/home/musil/git/torch_nl/torch_nl/neighbor_list.py)\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "torch.bincount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((518,),\n",
       " array([1.09439479, 1.09439479, 1.09439479, 1.09439479, 1.09439488,\n",
       "        1.09439488, 1.09439488, 1.09439488, 1.09439498, 1.09439498,\n",
       "        1.09439498, 1.09439498, 1.10242263, 1.10242263, 1.10242288,\n",
       "        1.10242288, 1.10242288, 1.10242288, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.34233938, 1.34233938, 1.34233938, 1.34233938, 1.34233938,\n",
       "        1.49342443, 1.49342443, 1.49342443, 1.49342443, 1.4934251 ,\n",
       "        1.4934251 , 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.73205081, 1.73205081, 1.73205081, 1.73205081,\n",
       "        1.73205081, 1.76549069, 1.76549069, 1.76549069, 1.76549069,\n",
       "        1.76549084, 1.76549084, 1.76549084, 1.76549084, 1.76549092,\n",
       "        1.76549092, 1.76549092, 1.76549092, 1.77408553, 1.77408553,\n",
       "        1.77408553, 1.77408553, 1.774086  , 1.774086  , 2.15029608,\n",
       "        2.15029608, 2.15029608, 2.15029608, 2.15029623, 2.15029623,\n",
       "        2.15029623, 2.15029623, 2.15029637, 2.15029637, 2.15029637,\n",
       "        2.15029637, 2.1566541 , 2.1566541 , 2.15665443, 2.15665443,\n",
       "        2.15665443, 2.15665443, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.19203102, 2.19203102,\n",
       "        2.19203102, 2.19203102, 2.19203102, 2.483128  , 2.483128  ,\n",
       "        2.48312806, 2.48312806, 2.48312806, 2.48312806, 2.560294  ,\n",
       "        2.560294  , 2.56029431, 2.56029431, 2.56029431, 2.56029431,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.57038421, 2.57038421, 2.57038421, 2.57038421,\n",
       "        2.57038421, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.64137748, 2.64137748,\n",
       "        2.64137748, 2.64137748, 2.64137748, 2.74867904, 2.74867904,\n",
       "        2.74867904, 2.74867904, 2.74867933, 2.74867933, 2.74867933,\n",
       "        2.74867933, 2.74867985, 2.74867985, 2.74867985, 2.74867985,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.82842712, 2.82842712, 2.82842712, 2.82842712, 2.82842712,\n",
       "        2.98224019, 2.98224019, 2.98224019, 2.98224019, 2.98224082,\n",
       "        2.98224082, 2.98224082, 2.98224082, 2.98224106, 2.98224106,\n",
       "        2.98224106, 2.98224106, 2.99903928, 2.99903928, 2.99903928,\n",
       "        2.99903928, 2.99904   , 2.99904   ]))"
      ]
     },
     "execution_count": 450,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping, batch_mapping, shifts_idx = strict_nl(rcut, pos, cell, mapping, batch_mapping, shifts_idx)\n",
    "cell_shifts = compute_cell_shifts(cell.view(-1,3,3), shifts_idx, batch_mapping)\n",
    "d = compute_distances(pos, mapping, cell_shifts).numpy()\n",
    "d = np.sort(d)\n",
    "d.shape,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.allclose(d, d_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 2, 4],\n",
       "        [3, 5, 8, 8]])"
      ]
     },
     "execution_count": 380,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.tensor([[1,4,2,1], [3,8,5,8]])\n",
    "torch.sort(aa, dim=1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], size=(0, 2), dtype=torch.int64)"
      ]
     },
     "execution_count": 403,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa = torch.tensor([0, 1]) \n",
    "bb = torch.tensor([], dtype=torch.int64)\n",
    "torch.cartesian_prod(aa,bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aa.abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1])"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def fff(cell, rc):\n",
    "    b1_c, b2_c, b3_c = np.linalg.pinv(cell).T\n",
    "\n",
    "    # Compute distances of cell faces.\n",
    "    l1 = np.linalg.norm(b1_c)\n",
    "    l2 = np.linalg.norm(b2_c)\n",
    "    l3 = np.linalg.norm(b3_c)\n",
    "    face_dist_c = np.array([1 / l1 if l1 > 0 else 1,\n",
    "                            1 / l2 if l2 > 0 else 1,\n",
    "                            1 / l3 if l3 > 0 else 1])\n",
    "    nbins_c = np.maximum((face_dist_c / rc).astype(int), [1, 1, 1])\n",
    "    return nbins_c\n",
    "cell = np.array([[6.19330899, 0.0, 0.0], [2.4074486111396207, 6.149627748674982, 0.0], [0.2117993724186579, 1.0208820183960539, 7.305899571570074]])\n",
    "\n",
    "\n",
    "fff(cell, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('cg39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "48.9333px",
    "width": "251.8px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  },
  "vscode": {
   "interpreter": {
    "hash": "f79d3df5ff5684964744ab9f5218f96eb946f2b40d3f02d5eb965bb50f364a25"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
